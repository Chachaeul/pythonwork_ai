{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR gate\n",
    "\n",
    " 서로 다른값일 때 참 , 같은 값이면 거짓 ex) 000 거짓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0,0], [0,0, 1], [0,1, 0], [0,1, 1],[1,0,0],[1,1,0],[1,0,1],[1,1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1],[1],[1],[1],[1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), dtype=tf.float32)\n",
    "b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X ,W) + b) \n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000):\n",
    "       sess.run(train, feed_dict={X: x_data, y: y_data})\n",
    "    \n",
    "h, p ,a = sess.run([hypot, pred, accuracy], feed_dict={X: x_data, y: y_data})\n",
    "print(\"가설 :\",h, \"\\n예측 :\",p, \"\\n정확도 :\",a)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), dtype=np.float32)\n",
    "b = tf.Variable(tf.random_normal([1]), dtype=np.float32)\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000) :\n",
    "    sess.run(train, feed_dict = {X : x_data, y : y_data})\n",
    "    \n",
    "h, p, a = sess.run([hypot, pred, accuracy],\n",
    "                   feed_dict = {X : x_data, y : y_data})\n",
    "print('가설 :', h, '\\n예측 :', p, '\\n정확도 :', a)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [0], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), dtype=np.float32)\n",
    "b = tf.Variable(tf.random_normal([1]), dtype=np.float32)\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000) :\n",
    "    sess.run(train, feed_dict = {X : x_data, y : y_data})\n",
    "    \n",
    "h, p, a = sess.run([hypot, pred, accuracy],\n",
    "                   feed_dict = {X : x_data, y : y_data})\n",
    "print('가설 :', h, '\\n예측 :', p, '\\n정확도 :', a)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [0], [1], [1], [1], [0]], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=100)\n",
    "clf.fit(x_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[1,1,1],[0,0,0],[0,0,1],[0,1,0]]\n",
    "test_label = [0,0,1,1]\n",
    "\n",
    "result=clf.predict(test)\n",
    "print(result)\n",
    "\n",
    "score = metrics.accuracy_score(test_label, result)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝을 이용한 XOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [0], [1], [1], [1], [0]], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "\n",
    "W1= tf.Variable(tf.random_normal([3, 10]), dtype=np.float32) # 출력의 갯수 10개\n",
    "b1 = tf.Variable(tf.random_normal([10]), dtype=np.float32)  #BIAS 도 출력의 갯수와 같아야한다\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1) #활성화 함수\n",
    "# 첫번째 히든계층의 출력갯수가 다음 계층의 입력갯수가 됨. 연결됨\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), dtype=np.float32) \n",
    "#10개 입력 받음 # 출력은 최종출력의 갯수를 그대로 따라서 1\n",
    "b2 = tf.Variable(tf.random_normal([1]), dtype=np.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(layer1 , W2) + b2) # layer1 (활성화 함수를 통과하여) 으로부터 입력받음\n",
    "\n",
    "#히든계층 하나, 두개\n",
    "\n",
    "# 딥러닝 = 멀티 레이어 퍼셉트론!\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10000) :\n",
    "    sess.run(train, feed_dict = {X : x_data, y : y_data})\n",
    "    \n",
    "h, p, a = sess.run([hypot, pred, accuracy],\n",
    "                   feed_dict = {X : x_data, y : y_data})\n",
    "print('가설 :', h, '\\n예측 :', p, '\\n정확도 :', a)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep & Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep : 6개의 hidden layer 추가\n",
    "# 각 계층의 입출력 개수는 50개\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0],       [1],       [1],       [1],       [1],       [1],       [1],       [0]      ], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3]) \n",
    "y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3, 50]), dtype = tf.float32)\n",
    "b1 = tf.Variable(tf.random_normal([50]), dtype = tf.float32)\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([50, 50]), dtype = tf.float32)\n",
    "b2 = tf.Variable(tf.random_normal([50]), dtype = tf.float32)\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1 ,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([50, 50]), dtype = tf.float32)\n",
    "b3 = tf.Variable(tf.random_normal([50]), dtype = tf.float32)\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2 ,W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([50, 50]), dtype = tf.float32)\n",
    "b4 = tf.Variable(tf.random_normal([50]), dtype = tf.float32)\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3 ,W4) + b4)\n",
    "\n",
    "W5 = tf.Variable(tf.random_normal([50, 50]), dtype = tf.float32)\n",
    "b5 = tf.Variable(tf.random_normal([50]), dtype = tf.float32)\n",
    "layer5 = tf.sigmoid(tf.matmul(layer4 ,W5) + b5)\n",
    "\n",
    "W6 = tf.Variable(tf.random_normal([50, 1]), dtype = tf.float32)\n",
    "b6 = tf.Variable(tf.random_normal([1]), dtype = tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(layer5 ,W6) + b6) \n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype = tf.float32))\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10000):\n",
    "       sess.run(train, feed_dict = {X : x_data, y : y_data})\n",
    "    \n",
    "h, p, a = sess.run([hypot, pred, accuracy], feed_dict = {X : x_data, y : y_data})\n",
    "print(\"가설 :\", h, \"\\n예측 :\", p, \"\\n정확도 :\", a)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0,0], [0,0, 1], [0,1, 0], [0,1, 1],[1,0,0],[1,1,0],[1,0,1],[1,1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,3])\n",
    "Y = tf.placeholder(tf.float32 ,shape=[None,1])\n",
    "\n",
    "# 보고자 하는 데이터를 그룹으로 묶음( 시각화했을 때 확인하기 쉬움 )\n",
    "# with문을 사용하여 tensorflow에 있는 name_scope모듈 사용\n",
    "# with tf.name_scope('layer1'):\n",
    "W1= tf.Variable(tf.random_normal([3,10]),dtype=tf.float32)\n",
    "b1 = tf.Variable(tf.random_normal([10]),dtype=tf.float32)\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "# 히든계층을 추가로 더해주는 것이 딥러닝(머신러닝 = 히든계층 1개, 딥러니 = 2개 이상)\n",
    "W2= tf.Variable(tf.random_normal([10,1]),dtype=tf.float32)\n",
    "b2 = tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "# 어떤 데이터를 어떻게 표현할지 정해놓음\n",
    "tf.summary.histogram('weight2',W2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(layer2)+(1-Y)*tf.log(1-layer2))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "tf.summary.scalar('cost',cost)\n",
    "\n",
    "pred = tf.cast(layer2>0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,Y),dtype=tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('log_dir2/alpha01')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for step in range(10000):\n",
    "    _,summary = sess.run([train,merged_summary], feed_dict={X:x_data,Y:y_data})\n",
    "    writer.add_summary(summary,global_step=step)\n",
    "\n",
    "l, p , a = sess.run([layer2,pred,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "print('가설 :\\n',l, '\\n예측 :\\n', p, '\\n정확도 :\\n', a)    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\Users\\user\\noeul\\pythonwork\\ai\n",
    "# tensorboard --logdir=./log_dir2/alpha01\n",
    "# http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha01 에 있는 그래프 다 지우고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape = [None,3])\n",
    "y = tf.placeholder(tf.float32,shape = [None,1])\n",
    "\n",
    "with tf.name_scope('layer1'):\n",
    "    W1 = tf.Variable(tf.random_normal([3,10]),dtype = tf.float32)\n",
    "    b1 = tf.Variable(tf.random_normal([10]),dtype = tf.float32)\n",
    "    layor1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weihgt1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"layor1\", layor1)\n",
    "\n",
    "with tf.name_scope('layer2'):\n",
    "    W2 = tf.Variable(tf.random_normal([10,1]),dtype = tf.float32)\n",
    "    b2 = tf.Variable(tf.random_normal([1]),dtype = tf.float32)\n",
    "    hypot = tf.sigmoid(tf.matmul(layor1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weihgt2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"hypot\", hypot)\n",
    "\n",
    "with tf.name_scope('cost'):\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypot) + (1-y)*tf.log(1-hypot))\n",
    "\n",
    "    tf.summary.scalar('cost',cost)\n",
    "    \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('log_dir2/alpha01')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for step in range(10000):\n",
    "    _, summary = sess.run([train, merged_summary], feed_dict={X:x_data,y:y_data})\n",
    "    writer.add_summary(summary, global_step = step)\n",
    "    \n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.01\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape = [None,3])\n",
    "y = tf.placeholder(tf.float32,shape = [None,1])\n",
    "\n",
    "with tf.name_scope('layer1'):\n",
    "    W1 = tf.Variable(tf.random_normal([3,10]),dtype = tf.float32)\n",
    "    b1 = tf.Variable(tf.random_normal([10]),dtype = tf.float32)\n",
    "    layor1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weihgt1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"layor1\", layor1)\n",
    "\n",
    "with tf.name_scope('layer2'):\n",
    "    W2 = tf.Variable(tf.random_normal([10,1]),dtype = tf.float32)\n",
    "    b2 = tf.Variable(tf.random_normal([1]),dtype = tf.float32)\n",
    "    hypot = tf.sigmoid(tf.matmul(layor1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weihgt2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"hypot\", hypot)\n",
    "\n",
    "with tf.name_scope('cost'):\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypot) + (1-y)*tf.log(1-hypot))\n",
    "\n",
    "    tf.summary.scalar('cost',cost)\n",
    "    \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('log_dir2/alpha001')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for step in range(10000):\n",
    "    _, summary = sess.run([train, merged_summary], feed_dict={X:x_data,y:y_data})\n",
    "    writer.add_summary(summary, global_step = step)\n",
    "    \n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\Users\\user\\noeul\\pythonwork\\ai\n",
    "#tensorboard --logdir=./log_dir2\n",
    "# http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network 2:\n",
    "\n",
    "이전 까지는 죽을뻔한 인공지능을 살려준  Backpropagation. 그러나 이것도 문제가 있다.\n",
    "\n",
    "해결 방안 Neural Network 2, ReLU(렐루)\n",
    "\n",
    "시그모이드 0-1 렐루는 비선형을 없앰 1이상.\n",
    "시그모이드는 비용이 안줄어든다. 렐루는 성능효과를 볼 수 있음\n",
    "\n",
    "렐루는 초기값문제가 있으므로 잘 주자. 0으로 주면 안돼. 랜덤으로 주는것도 무식. 오차가 가장 적을 수 있는 초기값을 찾자. - Restricted Boatman Machine (RBM) 알고리즘으로 초기값을 찾을 수 있다 더 쉽게 #  Xavier 초기화 ㄱㅏ능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-c73d85a5a6c4>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 90%\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28*28, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  레이어  3개 추가\n",
    "# 입출력 갯수 : 256\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.sigmoid(logit)   #중간중간에는 소프트 맥스(활성화 함수) 쓰면 안됨 분류니까 시그모이드 써\n",
    " \n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.sigmoid(logit)\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.sigmoid(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  레이어  3개 추가\n",
    "# 입출력 갯수 : 256\n",
    "# 시그모이드를 렐루로 바꿔보자 (다 렐루쓴다고 좋아지는건 아님. 마지막 은닉은 sigmoid로 조절)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)   #중간중간에는 소프트 맥스(활성화 함수) 쓰면 안됨 분류니까 시그모이드 써\n",
    " \n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.sigmoid(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Xavier 초기화\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784,256], initializer=tf.contrib.layers.xavier_initializer())  #랜덤으로 준 초기화는 지우고 자비에 초기화 \n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)   #중간중간에는 소프트 맥스(활성화 함수) 쓰면 안됨 분류니까 시그모이드 써\n",
    " \n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256,256], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256,256], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.sigmoid(logit)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-d13855cde193>:50: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "epoch: 1     cost: 1.853472747152502     accuracy: 0.56\n",
      "epoch: 2     cost: 0.4509136229211636     accuracy: 0.88\n",
      "epoch: 3     cost: 0.26943169247020377     accuracy: 0.98\n",
      "epoch: 4     cost: 0.21075397735292276     accuracy: 0.915\n",
      "epoch: 5     cost: 0.17120593882419843     accuracy: 0.95\n",
      "epoch: 6     cost: 0.14308135556903762     accuracy: 0.925\n",
      "epoch: 7     cost: 0.12244450409304021     accuracy: 0.975\n",
      "epoch: 8     cost: 0.10437386749820275     accuracy: 0.96\n",
      "epoch: 9     cost: 0.3957048673453656     accuracy: 0.95\n",
      "epoch: 10     cost: 0.1208172795989296     accuracy: 0.95\n",
      "epoch: 11     cost: 0.08415813671594308     accuracy: 0.97\n",
      "epoch: 12     cost: 0.07111597978255964     accuracy: 0.975\n",
      "epoch: 13     cost: 0.06155676859007643     accuracy: 0.97\n",
      "epoch: 14     cost: 0.05242540680210699     accuracy: 0.975\n",
      "epoch: 15     cost: 0.04495272538032044     accuracy: 0.99\n",
      "epoch: 16     cost: 0.21734867823225534     accuracy: 0.975\n",
      "epoch: 17     cost: 0.06415410140021281     accuracy: 0.985\n",
      "epoch: 18     cost: 0.04200994182547389     accuracy: 0.985\n",
      "epoch: 19     cost: 0.033395320138132026     accuracy: 0.995\n",
      "epoch: 20     cost: 0.02730198262005369     accuracy: 0.995\n",
      "epoch: 21     cost: 0.021410587080754342     accuracy: 0.99\n",
      "epoch: 22     cost: 0.30642756928605125     accuracy: 0.985\n",
      "epoch: 23     cost: 0.04775129399855029     accuracy: 0.99\n",
      "epoch: 24     cost: 0.028348077770153236     accuracy: 0.99\n",
      "epoch: 25     cost: 0.019835193736309348     accuracy: 1.0\n",
      "epoch: 26     cost: 0.015283145156942986     accuracy: 0.995\n",
      "epoch: 27     cost: 0.22183960707794703     accuracy: 0.865\n",
      "epoch: 28     cost: 0.06745581186630503     accuracy: 0.99\n",
      "epoch: 29     cost: 0.019720318960025914     accuracy: 0.995\n",
      "epoch: 30     cost: 0.011803541641513055     accuracy: 1.0\n",
      "훈련 종료\n",
      "정확도 :  0.9792\n"
     ]
    }
   ],
   "source": [
    "#  layer.는 총 8개로 구성.\n",
    "# 입출력 갯수는 512개\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784,512], initializer=tf.contrib.layers.xavier_initializer())  #랜덤으로 준 초기화는 지우고 자비에 초기화 \n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)   #중간중간에는 소프트 맥스(활성화 함수) 쓰면 안됨 분류니까 시그모이드 써\n",
    " \n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5= tf.nn.relu(logit)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.nn.relu(logit)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512,10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels, prob:1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop out  : 과적합을 해결하는 법\n",
    "sess 실행할때 옵션만 추가하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 0.932098634297198     accuracy: 0.725\n",
      "epoch: 2     cost: 0.24744749602946367     accuracy: 0.73\n",
      "epoch: 3     cost: 0.17859888748689132     accuracy: 0.675\n",
      "epoch: 4     cost: 0.14652779866348611     accuracy: 0.645\n",
      "epoch: 5     cost: 0.1264511896128005     accuracy: 0.635\n",
      "epoch: 6     cost: 0.11160971647636451     accuracy: 0.655\n",
      "epoch: 7     cost: 0.10486831110986795     accuracy: 0.68\n",
      "epoch: 8     cost: 0.09209607188674537     accuracy: 0.725\n",
      "epoch: 9     cost: 0.0865175271813165     accuracy: 0.685\n",
      "epoch: 10     cost: 0.07921921696175228     accuracy: 0.735\n",
      "epoch: 11     cost: 0.0781078055162322     accuracy: 0.675\n",
      "epoch: 12     cost: 0.07156356693668803     accuracy: 0.735\n",
      "epoch: 13     cost: 0.0664901121299375     accuracy: 0.725\n",
      "epoch: 14     cost: 0.05929155797443608     accuracy: 0.73\n",
      "epoch: 15     cost: 0.05788185272196476     accuracy: 0.735\n",
      "epoch: 16     cost: 0.06297747434032235     accuracy: 0.7\n",
      "epoch: 17     cost: 0.05698548518290576     accuracy: 0.635\n",
      "epoch: 18     cost: 0.05233187161555343     accuracy: 0.675\n",
      "epoch: 19     cost: 0.0566009651954201     accuracy: 0.685\n",
      "epoch: 20     cost: 0.05016972881149164     accuracy: 0.72\n",
      "epoch: 21     cost: 0.05489582317965949     accuracy: 0.72\n",
      "epoch: 22     cost: 0.05047705896706741     accuracy: 0.65\n",
      "epoch: 23     cost: 0.04331981301032517     accuracy: 0.68\n",
      "epoch: 24     cost: 0.04497873866083947     accuracy: 0.735\n",
      "epoch: 25     cost: 0.04747207912447102     accuracy: 0.725\n",
      "epoch: 26     cost: 0.04528533544306727     accuracy: 0.685\n",
      "epoch: 27     cost: 0.04475464464842599     accuracy: 0.7\n",
      "epoch: 28     cost: 0.047256713549352514     accuracy: 0.66\n",
      "epoch: 29     cost: 0.04123413183345375     accuracy: 0.72\n",
      "epoch: 30     cost: 0.04049408216495066     accuracy: 0.75\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "#  DropOut\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "prob=tf.placeholder(tf.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784,512], initializer=tf.contrib.layers.xavier_initializer())  #랜덤으로 준 초기화는 지우고 자비에 초기화 \n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)   #중간중간에는 소프트 맥스(활성화 함수) 쓰면 안됨 분류니까 시그모이드 써\n",
    "layer1 = tf.nn.dropout(layer1 , keep_prob=prob )   #512라는 교차 학습이 일어날텐데 0.7은 쓰고 0.3은 버리게\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "layer2 = tf.nn.dropout(layer2 , keep_prob=prob )\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.relu(logit)\n",
    "layer3 = tf.nn.dropout(layer3 , keep_prob=prob )\n",
    "\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit)\n",
    "layer4 = tf.nn.dropout(layer4 , keep_prob=prob )\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5= tf.nn.relu(logit)\n",
    "layer5 = tf.nn.dropout(layer5 , keep_prob=prob )\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit)\n",
    "layer6 = tf.nn.dropout(layer6 , keep_prob=prob )\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.nn.relu(logit)\n",
    "layer7 = tf.nn.dropout(layer7 , keep_prob=prob )\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512,10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "hypothesis = tf.nn.dropout(hypothesis , keep_prob=prob )\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)   \n",
    "#   미분을 활용한 train.GradientDescentOptimize 보다 tf.train.AdamOptimizer 성능이 더 좋다. 일반적으로\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost, accuracy], feed_dict={X:batch_xs, y:batch_ys,prob:0.7}) \n",
    "        #플레이스홀더에 넣는 것. 따라서 추가할때 위에 변수 만들어줘야함 #70%만 훈련하고 30%는 버리겠다\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"    accuracy:\", acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9827\n"
     ]
    }
   ],
   "source": [
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels, prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
